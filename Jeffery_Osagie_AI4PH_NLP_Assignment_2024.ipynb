{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment\n",
    "\n",
    "### **Jeffery Osagie**  \n",
    "\n",
    "#### AI4PH\n",
    "\n",
    "##### June, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook successfully completes several basic Natural Language Processing (NLP) tasks using the Brown Corpus from the Natural Language Toolkit (NLTK) library. It covers raw text data preprocessing, part-of-speech tagging, and frequency analysis of tokens. Tasks completed include:\n",
    "\n",
    "1. Loading the Brown Corpus from NLTK using the corpus reader function `paras()` for paragraphs. \n",
    "\n",
    "\n",
    "2. Removing punctuation and stopwords.\n",
    "\n",
    "\n",
    "3. Applying the Lancaster Stemmer - reducing tokens to their base or root form. \n",
    "\n",
    "\n",
    "4. Calculating Term frequency (TF) for the entire corpus and printing the top 10 words with the highest TF values. \n",
    "\n",
    "\n",
    "5. Calculating and Displaying the top 10 words in terms of term frequency-inverse document frequency (TF-IDF). Using paragraphs as documents for calculating TF-IDF. \n",
    "\n",
    "\n",
    "6. Part-of-Speech (POS) Tagging of Tokens. Tokens are tagged with their respective parts of speech using the `pos_tag` function from NLTK.\n",
    "\n",
    "\n",
    "7. Printing the 10 most common trigrams of word-tag pairs with their frequencies as well, using `nltk.trigrams()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries and Download necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import relevant libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the Brown Corpus\n",
    "\n",
    "The `paras()` function returns the given file(s) as a list of paragraphs, each encoded as a list of sentences, which are in turn encoded as lists of word string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']], [['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]]\n"
     ]
    }
   ],
   "source": [
    "# Load Brown Corpus paragraphs\n",
    "brown_paras = brown.paras()\n",
    "\n",
    "# Print the first two paragraphs\n",
    "print(brown_paras[:2]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten the list of list of paragraphs into a list of strings because it simplifies the data structure, making it easier to work with and more compatible with many natural language processing libraries and tools that expect text data in a specific format. Additionally, it can be a useful preprocessing step for tasks like tokenization, language modeling, and creating bag-of-words representations, where treating the entire text corpus as a continuous sequence of tokens is often required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\", \"The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\"]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists of sentences into a list of paragraphs\n",
    "paras = [' '.join([' '.join(sent) for sent in para]) for para in brown_paras]\n",
    "\n",
    "# Print first two paragraphs of flattened list\n",
    "print(paras[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removing Punctuation and Stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The text is then tokenized, converted to lowercase, and filtered to remove **punctuation** and **stopwords**. \n",
    " \n",
    " **Tokenization** is crucial because it breaks the text into individual units (tokens/words) that can be processed and analyzed.\n",
    " \n",
    " **Stopwords** are common words such as \"*and*,\" \"*the*,\" \"*is*,\" and \"*in*\" that are typically filtered out during text processing. These words are usually removed because they are considered to carry little meaningful information and can clutter the analysis.\n",
    " \n",
    " **Stemming** reduces words to their root forms, which can improve the ability to match and group related words together, enhancing the performance of tasks like document classification or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text: remove punctuation and stopwords, and apply stemming\n",
    "def preprocess(text, stemmer):\n",
    "    # Convert to lowercase and remove non-word characters\n",
    "    text_lower = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_lower)\n",
    "        \n",
    "    # Remove stopwords, punctuation, hyphens, numbers, past tenses, 2-letter words and perform stemming\n",
    "    filtered_tokens = [stemmer.stem(token) for token in tokens\n",
    "                       if token not in stop_words\n",
    "                       and token not in punctuation\n",
    "                       and not re.search(r'-', token)\n",
    "                       and not re.search(r'^\\d', token)\n",
    "                       and not re.search(r'ed$', token)\n",
    "                       and len(token) >= 3]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying the Lancaster Stemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lancaster Stemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Preprocess each paragraph\n",
    "processed_paras = [preprocess(para, lancaster) for para in paras]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Term Frequencies**\n",
    "Calculating term frequency (TF) is a fundamental step in many natural language processing tasks as it quantifies how important a given word is within a document or corpus based on its frequency of occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculating and Displaying the Top 10 Words by Term Frequency (TF)\n",
    "Printing out the top 10 words by term frequency can provide insights into the main topics or themes present in the text data, which can be valuable for tasks like topic modeling, document clustering, or keyword extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words by Term Frequency (TF):\n",
      "on: 3501\n",
      "would: 2715\n",
      "stat: 2061\n",
      "said: 1961\n",
      "tim: 1953\n",
      "ev: 1946\n",
      "man: 1796\n",
      "new: 1785\n",
      "year: 1689\n",
      "could: 1602\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists into a single list of tokens for TF calculation\n",
    "all_tokens = [token for para in processed_paras for token in para]\n",
    "\n",
    "# Calculate term frequency (TF)\n",
    "tf_dist = FreqDist(all_tokens)\n",
    "top_10_tf = tf_dist.most_common(10)\n",
    "\n",
    "# Print top 10 words in terms of TF and their values\n",
    "print(\"Top 10 words by Term Frequency (TF):\")\n",
    "for word, freq in top_10_tf:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating and Displaying the Top 10 Words by Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "To identify the relevance of a word in a document relative to a collection of documents, we weigh its frequency in the document against its rarity across the corpus. This allows for better feature extraction and improves the performance of information retrieval and text mining tasks by emphasizing significant terms and downplaying common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words by TF-IDF:\n",
      "said: 315.14951335278766\n",
      "on: 186.13514891683818\n",
      "would: 173.55228489152353\n",
      "man: 152.72032435666907\n",
      "stat: 148.158996245877\n",
      "tim: 140.29478480601392\n",
      "get: 139.62499718459867\n",
      "year: 137.1806091683378\n",
      "ev: 136.2384551909466\n",
      "new: 134.67350845426412\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF\n",
    "def compute_tf_idf(corpus):\n",
    "    num_docs = len(corpus)\n",
    "    idf_dict = {}\n",
    "    tf_idf_dict = {}\n",
    "    \n",
    "    for doc in corpus:\n",
    "        for word in set(doc):\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    \n",
    "    for word, doc_count in idf_dict.items():\n",
    "        idf_dict[word] = math.log(num_docs / float(doc_count))\n",
    "    \n",
    "    for doc in corpus:\n",
    "        word_counts = FreqDist(doc)\n",
    "        doc_len = len(doc)\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / float(doc_len)\n",
    "            idf = idf_dict[word]\n",
    "            tf_idf = tf * idf\n",
    "            tf_idf_dict[word] = tf_idf_dict.get(word, 0) + tf_idf \n",
    "    \n",
    "    return tf_idf_dict\n",
    "\n",
    "\n",
    "tf_idf_dict = compute_tf_idf(processed_paras)\n",
    "sorted_tf_idf = sorted(tf_idf_dict.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "\n",
    "# Print top 10 words in terms of TF-IDF and their values\n",
    "print(\"\\nTop 10 words by TF-IDF:\")\n",
    "for word, score in sorted_tf_idf:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part-of-Speech (POS) tagging**\n",
    " To understand the grammatical structure and meaning of a sentence, we identify and label the parts of speech (nouns, verbs, adjectives, etc.) for each word. This enhances tasks like syntactic parsing, named entity recognition, and sentiment analysis by providing context and disambiguating the roles words play within sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Part-of-Speech (POS) Tagging of Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag tokens with part-of-speech tags\n",
    "tagged_tokens = [pos_tag(para) for para in processed_paras]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Calculating and Displaying the Most Common Trigrams of Word-Tag Pairs\n",
    "Trigram word pairs, also known simply as trigrams, are sequences of three consecutive words in a text. For example, in the sentence \"The quick brown fox,\" the trigrams would be \"The quick brown\" and \"quick brown fox.\" Each trigram captures a snapshot of the local context within the text. They are used to capture more contextual information than individual words or bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most common trigrams of word-tag pairs:\n",
      "(('new', 'JJ'), ('york', 'NN'), ('city', 'NN')): 28\n",
      "(('new', 'JJ'), ('york', 'NN'), ('tim', 'NN')): 23\n",
      "(('index', 'NN'), ('word', 'NN'), ('electron', 'NN')): 21\n",
      "(('word', 'NN'), ('electron', 'NN'), ('switch', 'NN')): 20\n",
      "(('new', 'JJ'), ('york', 'NN'), ('cent', 'NN')): 15\n",
      "(('drug', 'NN'), ('chem', 'NN'), ('nam', 'FW')): 14\n",
      "(('per', 'IN'), ('capit', 'NN'), ('incom', 'NN')): 14\n",
      "(('two', 'CD'), ('year', 'NN'), ('ago', 'RB')): 13\n",
      "(('john', 'NN'), ('not', 'RB'), ('govern', 'JJ')): 11\n",
      "(('per', 'IN'), ('head', 'NN'), ('dai', 'NN')): 10\n"
     ]
    }
   ],
   "source": [
    "# Create trigrams of word-tag pairs\n",
    "trigrams_list = [trigrams(para) for para in tagged_tokens]\n",
    "flat_trigrams = [trigram for sublist in trigrams_list for trigram in sublist]\n",
    "\n",
    "# Calculate frequency distribution of trigrams\n",
    "trigrams_freq = FreqDist(flat_trigrams)\n",
    "top_10_trigrams = trigrams_freq.most_common(10)\n",
    "\n",
    "# Print top 10 trigrams and their frequencies\n",
    "print(\"\\nTop 10 most common trigrams of word-tag pairs:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
